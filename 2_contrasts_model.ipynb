{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 constrasts model\n",
    "\n",
    "This notebook load, preprocess the data and train a first modèle to predict if a 2 image is T1w or T2w.\n",
    "The Notebook form helps running and testing fast before coding the final structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from monai.data import Dataset, DataLoader, CacheDataset\n",
    "import torchvision.models as models\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandScaleCrop,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandRotate,\n",
    "    RandShiftIntensityd,\n",
    "    ToTensord,\n",
    "    \n",
    ")\n",
    "import os\n",
    "import nibabel as nib\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import monai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for T1w, T2w, and DWI files in data//data-multi-subject// ...\n",
      "Found 20 T1w files and 20 T2w files.\n"
     ]
    }
   ],
   "source": [
    "# this cell aims at extracting the list of path relevant for the first model test which takes T1w T2w adn DWI as input\n",
    "\n",
    "base_dir=\"data//data-multi-subject//\"\n",
    "\n",
    "desired_extension = \".json\"\n",
    "\n",
    "# Initialize lists to store the relative paths for T1w, T2w, and DWI files\n",
    "t1w_file_paths = []\n",
    "t2w_file_paths = []\n",
    "\n",
    "print(\"Searching for T1w, T2w, and DWI files in\", base_dir, \"...\")\n",
    "\n",
    "# Traverse the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    # Exclude the \"derivatives\" subfolder\n",
    "    if \"derivatives\" in dirs:\n",
    "        dirs.remove(\"derivatives\")\n",
    "    for file in files:\n",
    "        # Check if the file name contains the desired names\n",
    "        if \"T1w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T1w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T1w file paths list\n",
    "            t1w_file_paths.append(relative_path)\n",
    "        elif \"T2w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T2w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T2w file paths list\n",
    "            t2w_file_paths.append(relative_path)\n",
    "\n",
    "t1w_file_paths = t1w_file_paths[:20]\n",
    "t2w_file_paths = t2w_file_paths[:20]\n",
    "\n",
    "print(\"Found\", len(t1w_file_paths), \"T1w files and\", len(t2w_file_paths), \"T2w files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load image data\n",
    "def load_image(image_path):\n",
    "    img = nib.load(image_path)\n",
    "    img_data = img.get_fdata()\n",
    "    return img_data\n",
    "\n",
    "def From_3D_to_2Ds(image, label):\n",
    "    #from each 3D image we will extract 10 2D images, from random different views\n",
    "    data_2D=[]\n",
    "    selected_perax=[[],[],[]] #list of the selected indexes for each view\n",
    "    for i in range(10):\n",
    "        # randomly select a view\n",
    "        view = np.random.choice([0, 1, 2]) # 0: sagittal, 1: coronal, 2: axial\n",
    "        # randomly select an index for wich we will extract the 2D image among the index which were not yet selected\n",
    "        index = np.random.choice([i for i in range(image.shape[view]) if i not in selected_perax[view]])\n",
    "        # choose randomly if the image is cropped or not  \n",
    "\n",
    "        if view == 0:\n",
    "            image_2D = image[index, :, :]\n",
    "        elif view == 1:\n",
    "            image_2D = image[:, index, :]\n",
    "        else:\n",
    "            image_2D = image[:, :, index]\n",
    "        selected_perax[view].append(index)\n",
    "        data_2D.append({\"image\" : image_2D, \"label\" : label})\n",
    "    return(data_2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            image_path   label\n",
      "0  sub-amu01\\anat\\sub-amu01_T1w.nii.gz  [1, 0]\n",
      "1  sub-amu02\\anat\\sub-amu02_T1w.nii.gz  [1, 0]\n",
      "2  sub-amu03\\anat\\sub-amu03_T1w.nii.gz  [1, 0]\n",
      "3  sub-amu04\\anat\\sub-amu04_T1w.nii.gz  [1, 0]\n",
      "4  sub-amu05\\anat\\sub-amu05_T1w.nii.gz  [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and validation sets\n",
    "\n",
    "# build a dataset with a colmn \"file path\" wich contiains the paths listed in both t1w_file_paths and t2w_file_paths\n",
    "path_data = pd.DataFrame({\"image_path\" : t1w_file_paths + t2w_file_paths, \"label\" : len(t1w_file_paths) * [[1,0]] + len(t2w_file_paths) * [[0,1]]})\n",
    "print(path_data.head())\n",
    "\n",
    "train_data, val_data = train_test_split(path_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# load the 3D images\n",
    "train_data_2D = []\n",
    "val_data_2D = []\n",
    "for index, row in train_data.iterrows():\n",
    "    image_path = os.path.join(base_dir, row[\"image_path\"])\n",
    "    image = load_image(image_path)\n",
    "    train_data_2D += From_3D_to_2Ds(image, row[\"label\"])\n",
    "for index, row in val_data.iterrows():\n",
    "    image_path = os.path.join(base_dir, row[\"image_path\"])\n",
    "    image = load_image(image_path)\n",
    "    val_data_2D += From_3D_to_2Ds(image, row[\"label\"])\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(train_data_2D)\n",
    "np.random.shuffle(val_data_2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Dataset_2D(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_item = self.data[index]\n",
    "        image = data_item[\"image\"]\n",
    "        label = data_item[\"label\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "# use monai to define the transforms for data augmentation\n",
    "# perform the following transformations : rotation (random between +3° and -3°), flipping (random between 0°,  90 °, 180° and 270°), cropping (Random size, random place) and shifting (random shift)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandRotate90d(keys=[\"image\"], prob=0.5),\n",
    "        RandFlipd(keys=[\"image\"], prob=0.5),\n",
    "        RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n",
    "        RandScaleCrop([np.random.uniform(0.5, 1), np.random.uniform(0.5,1)], random_center=True),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training and validation datasets\n",
    "train_dataset = Dataset_2D(train_data_2D, transform=train_transforms)\n",
    "val_dataset = Dataset_2D(val_data_2D, transform=train_transforms)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset.data, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset.data, batch_size=10)\n",
    "\n",
    "train_dataset.data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5767, 0.5020],\n",
      "        [0.8802, 0.8488],\n",
      "        [0.0636, 0.6604]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)  \u001b[38;5;66;03m# torch.Size([5, 2]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#print(output.squeeze(-1).shape)  # torch.Size([2, 4])\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ResNet18SingleChannel(nn.Module):\n",
    "    # Define the ResNet18 model with a single input channel and an output value between 0 and 1\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18SingleChannel, self).__init__()\n",
    "        # Load the pre-trained ResNet18 model\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # Modify the first convolutional layer to take a single channel input\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        # Modify the final fully connected layer to output a single value\n",
    "\n",
    "        self.resnet = resnet\n",
    "\n",
    "        #final fc to go from [batch_size, 1000] to [batch_size, num_classes]\n",
    "        self.fc = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet18SingleChannel(num_classes=2).to(device)\n",
    "\n",
    "#output = model.forward(torch.randn(3, 1, 49, 29))\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "print(f\"Validation accuracy: {correct / total}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
